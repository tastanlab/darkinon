phosphosite:
  dataset:
    train: dataset/new_dataset/zsl/seed_12345/train_data.csv
    validation: dataset/new_dataset/zsl/seed_12345/validation_data.csv
    test: dataset/new_dataset/zsl/seed_12345/test_data.csv
    train_val: dataset/new_dataset/zsl/seed_12345/train_val_data.csv
    processor:
      processor_type: hf
      model_name: esm1b_t33_650M_UR50S
      read_embeddings: false
      phosphosite_embedding_path: /truba/home/mpekey/dkz_models/esm1b_phosphosite.pt
      protvec_file_path: dataset/new_dataset/protvec_embeddings.txt
      split_multilabel_rows: false
  model:
    model_type: hf
    model_name: esm1b_t33_650M_UR50S
    embedding_mode: cls
    is_pretrained: false
    freeze: false
    unfrozen_layers: ''
    lora: false
    remove_layers: 0
    plot_residue_attentions: true
  
  sequence_model:
    model_type: bilstm
    hidden_size: 512
    use_sequence_model: false

kinase:
  dataset:
    train: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv
    validation: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv
    test: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv
    train_val: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv
    processor:
      processor_type: hf
      model_name: esm1b_t33_650M_UR50S
      read_embeddings: true
      kinase_embedding_path: /truba/home/mpekey/dkz_models/esm1b_kinase.pt
      protvec_file_path: dataset/new_dataset/protvec_embeddings.txt
      use_family: true
      use_group: true
      use_enzymes: true
      use_kin2vec: false
      use_pathway: false
      active_site:
        use_active_site: false
        from_context: false
        embedding_mode: cls
      use_domain: false
  model:
    model_type: hf
    model_name: esm1b_t33_650M_UR50S
    embedding_mode: cls
    is_pretrained: true
    freeze: true
    unfrozen_layers: ''
    lora: false

training:
  train_batch_size: 128
  test_batch_size: 128
  num_epochs: 100
  normalize_phosphosite_data: false
  normalize_kinase_data: false
  save_model: true
  set_seed: true
  precision: '32-true'
  loss_function: 'normalized_cross_entropy'

hyper_parameters:
  gamma : 0.97
  learning_rate : 0.01
  optimizer : 'SGD'
  scheduler_type : 'CosineAnnealingLR'
  weight_decay: 0.0001
  temperature: 1.0
  focal_loss_gamma: 1.0
  use_weighted_loss: false
  loss_weight_type: 'pairwise_sim'
  use_soft_probs_ce: false

logging:
  wandb:
    use_wandb : false
    log_name : 'esm1b_ts'
    project_name : 'transformer-training'
    entity_name : 'deepkinzero'
  local:
    run_test_suite: false
    use_config_filename: false
    save_predictions: false
    checkpoint_file_name: 'esm1b_ts'
    saved_model_path: checkpoints/esm1b_models
    kinase_encoder_save_path : checkpoints/esm1b_models/ke_esm1b_ts.pkl

lora_config:
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_query: true
  lora_key: true
  lora_value: true
  lora_output: true
  lora_intermediate: true