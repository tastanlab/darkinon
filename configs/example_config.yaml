phosphosite:
  dataset:
    train: dataset/new_dataset/zsl/seed_12345/train_data.csv # Train phosphosite data path
    validation: dataset/new_dataset/zsl/seed_12345/validation_data.csv # Validation phosphosite data path
    test: dataset/new_dataset/zsl/seed_12345/test_data.csv # Test phosphosite data path
    train_val: dataset/new_dataset/zsl/seed_12345/train_val_data.csv # Train and validation phosphosite data path
    processor:
      processor_type: hf # Processor type to preprocess sequences
      model_name: esm2_t6_8M_UR50D # Different models may require different processings
      read_embeddings: false # Do we read precomputed embeddings from file
      phosphosite_embedding_path: /truba/home/mpekey/dkz_models/esm1b_phosphosite.pt # File to read precomputed embeddings
      protvec_file_path: dataset/new_dataset/protvec_embeddings.txt
      split_multilabel_rows: true # split training multi-label rows to single label rows
  model:
    model_type: hf # Model type to create phosphosite embeddings while training
    model_name: esm2_t6_8M_UR50D # Model name to load or initialize
    embedding_mode: cls # Do we use all sequence, cls token embedding or sequence average embedding [sequence, cls, avg]
    is_pretrained: true # Do we use model with pretrained weights or initialize model from scratch
    freeze: true # Freeze all model layers
    unfrozen_layers: '' # Which layers will be unfrozen. Example: '1,2' will unfroze 1st and 2nd layers in encoders.layer
    lora: false # Use lora in linear layers
    remove_layers: 0 # Remove last n layers from the model
    plot_residue_attentions: false # Plot residue attentions for each sequence for test
  
  sequence_model:
    model_type: bilstm # Model to train on sequences
    hidden_size: 512 # Model hidden dimension
    use_sequence_model: false # Use a model to process sequence or not

kinase:
  dataset:
    train: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv # Train kinase data path
    validation: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv # Validation kinase data path
    test: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv # Test kinase data path
    train_val: dataset/new_dataset/zsl/seed_12345/kinase_properties.csv # Train validation kinase data path
    processor:
      processor_type: hf # Processor type to preprocess sequences
      model_name: esm2_t6_8M_UR50D # Different models may require different processings
      read_embeddings: true # Do we read precomputed embeddings from file
      kinase_embedding_path: /truba/home/mpekey/dkz_models/esm1b_kinase.pt # File to read precomputed embeddings
      protvec_file_path: dataset/new_dataset/protvec_embeddings.txt
      use_family: false # Use kinase family one hot encoding as extra feature
      use_group: false # Use kinase group one hot encoding as extra feature
      use_enzymes: false # Use ec classes as extra feature
      use_kin2vec: false # Use kin2vec (a kind of protvec embeddings) as extra feature
      use_pathway: false # Use kinase pathways as extra feature
      active_site:
        use_active_site: false # Use kinase active sites (can be used with domain or alone)
        from_context: false # Use all kinase sequence context to generate active site embeddings
        embedding_mode: cls # # Do we use all sequence, cls token embedding or sequence average embedding [cls, avg, cls_avg] 
      use_domain: true # Use kinase sequence create embeddings
  model:
    model_type: hf # Model type to create kinase embeddings while training
    model_name: esm2_t6_8M_UR50D # Model name to load or initialize
    embedding_mode: cls # Do we use all sequence, cls token embedding or sequence average embedding [sequence, cls, avg]
    is_pretrained: true # Do we use model with pretrained weights or initialize model from scratch
    freeze: true # Freeze all model layers
    unfrozen_layers: '' # Which layers will be unfrozen. Example: '1,2' will unfroze 1st and 2nd layers in encoders.layer
    lora: false # Use lora in linear layers

training:
  train_batch_size: 32
  test_batch_size: 32
  num_epochs: 2
  normalize_phosphosite_data: false # Normalize phosphosite data using embed_scaler_phosphosite.pkl file in checkpoints/model_folder
  normalize_kinase_data: false # Normalize phosphosite data using embed_scaler_kinase.pkl file in checkpoints/model_folder
  save_model: true # Save model in every epoch by validation aupr
  set_seed: true # If true, model ids will be used as seed
  precision: '32-true' # '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16'
  loss_function: 'normalized_cross_entropy' # 'cross_entropy', 'normalized_cross_entropy', 'focal_loss',

hyper_parameters:
  gamma : 0.95
  learning_rate : 0.015
  optimizer : 'SGD'
  scheduler_type : 'StepLR'
  weight_decay: 0.0005
  temperature: 1.0
  focal_loss_gamma: 1.0
  use_weighted_loss: false
  loss_weight_type: 'pairwise_sim' # 'pairwise_sim', 'family', 'group' - pairwise_sim uses train class counts. family uses train class counts of families
  use_soft_probs_ce: false

logging:
  wandb:
    use_wandb : false
    log_name : 'esm1b-transformer' # Wandb log name in Runs interface
    project_name : 'transformer-training' # Wandb project name to log
    entity_name : 'deepkinzero' # Wandb team name
  local:
    run_test_suite: false # Run test suite after training and testing
    use_config_filename: false # Add hyper_parameters to the saved model filename
    save_predictions: false # Save predictions on the test set to a csv file (class, family, group prediction probabilities)
    checkpoint_file_name: 'transformer-train' # Default saved model filename
    saved_model_path: checkpoints/esm1b_models
    kinase_encoder_save_path : checkpoints/esm1b_models/kinase_encoder_transformer.pkl

lora_config:
  lora_rank: 8 # Rank of the lora matrix
  lora_alpha: 16 # Alpha value for lora
  lora_dropout: 0.05 # Dropout value for lora
  lora_query: true # Use lora in query
  lora_key: true # Use lora in key
  lora_value: true # Use lora in value
  lora_output: true # Use lora in output
  lora_intermediate: true # Use lora in intermediate